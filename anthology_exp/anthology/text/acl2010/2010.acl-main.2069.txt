    Learning Better Data Representation using Inference-Driven Metric
                               Learning

     Paramveer S. Dhillon         Partha Pratim Talukdarâˆ—           Koby Crammer
    CIS Deptt., Univ. of Penn. Search Labs, Microsoft Research Deptt. of Electrical Engg.
     Philadelphia, PA, U.S.A      Mountain View, CA, USA       The Technion, Haifa, Israel
     dhillon@cis.upenn.edu                     partha@talukdar.net                      koby@ee.technion.ac.il




                       Abstract                                           datasets has not been studied before. In
                                                                          this paper, we address that gap: we com-
    We initiate a study comparing effective-                              pare effectiveness of classifiers trained on the
    ness of the transformed spaces learned by                             transformed spaces learned by metric learn-
    recently proposed supervised, and semi-                               ing methods to those generated by previ-
    supervised metric learning algorithms                                 ously proposed unsupervised dimensionality
    to those generated by previously pro-                                 reduction methods. We find IDML-IT, a
    posed unsupervised dimensionality reduc-                              semi-supervised metric learning algorithm to
    tion methods (e.g., PCA). Through a va-                               be the most effective.
    riety of experiments on different real-
    world datasets, we find IDML-IT, a semi-                        2     Metric Learning
    supervised metric learning algorithm to be
    the most effective.                                             2.1    Relationship between Metric Learning
                                                                           and Linear Projection
1   Introduction
                                                                    We first establish the well-known equivalence be-
Because of the high-dimensional nature of NLP                       tween learning a Mahalanobis distance measure
datasets, estimating a large number of parameters                   and Euclidean distance in a linearly transformed
(a parameter for each dimension), often from a                      space of the data (Weinberger and Saul, 2009). Let
limited amount of labeled data, is a challenging                    A be a d Ã— d positive definite matrix which param-
task for statistical learners. Faced with this chal-                eterizes the Mahalanobis distance, dA (xi , xj ), be-
lenge, various unsupervised dimensionality reduc-                   tween instances xi and xj , as shown in Equation
tion methods have been developed over the years,                    1. Since A is positive definite, we can decompose
e.g., Principal Components Analysis (PCA).                          it as A = P > P , where P is another matrix of size
   Recently, several supervised metric learning al-                 d Ã— d.
gorithms have been proposed (Davis et al., 2007;
Weinberger and Saul, 2009). IDML-IT (Dhillon et
                                                                        dA (xi , xj ) = (xi âˆ’ xj )> A(xi âˆ’ xj )       (1)
al., 2010) is another such method which exploits                                                       >
labeled as well as unlabeled data during metric                                     = (P xi âˆ’ P xj ) (P xi âˆ’ P xj )
learning. These methods learn a Mahalanobis dis-                                    = dEuclidean (P xi , P xj )
tance metric to compute distance between a pair
of data instances, which can also be interpreted as                    Hence, computing Mahalanobis distance pa-
learning a transformation of the input data, as we                  rameterized by A is equivalent to first projecting
shall see in Section 2.1.                                           the instances into a new space using an appropriate
   In this paper, we make the following contribu-                   transformation matrix P and then computing Eu-
tions:                                                              clidean distance in the linearly transformed space.
      Even though different supervised and semi-                    In this paper, we are interested in learning a better
      supervised metric learning algorithms have                    representation of the data (i.e., projection matrix
      recently been proposed, effectiveness of the                  P ), and we shall achieve that goal by learning the
      transformed spaces learned by them in NLP                     corresponding Mahalanobis distance parameter A.
    âˆ—
      Research carried out while at the University of Penn-            We shall now review two recently proposed
sylvania, Philadelphia, PA, USA.                                    metric learning algorithms.


                                                              377
                        Proceedings of the ACL 2010 Conference Short Papers, pages 377â€“381,
                  Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


2.2    Information-Theoretic Metric Learning                 xi is labeled. m is the total number of labels. Y
       (ITML): Supervised                                    is the n Ã— m matrix storing training label informa-
Information-Theoretic Metric Learning (ITML)                 tion, if any. YÌ‚ is the n Ã— m matrix of estimated la-
(Davis et al., 2007) assumes the availability of             bel information, i.e., output of any classifier, with
prior knowledge about inter-instance distances. In           YÌ‚il denoting score of label l at node i. .
this scheme, two instances are considered simi-                  The ITML metric learning algorithm, which we
lar if the Mahalanobis distance between them is              reviewed in Section 2.2, is supervised in nature,
upper bounded, i.e., dA (xi , xj ) â‰¤ u, where u              and hence it does not exploit widely available un-
is a non-trivial upper bound. Similarly, two in-             labeled data. In this section, we review Infer-
stances are considered dissimilar if the distance            ence Driven Metric Learning (IDML) (Algorithm
between them is larger than certain threshold l,             1) (Dhillon et al., 2010), a recently proposed met-
i.e., dA (xi , xj ) â‰¥ l. Similar instances are rep-          ric learning framework which combines an exist-
resented by set S, while dissimilar instances are            ing supervised metric learning algorithm (such as
represented by set D.                                        ITML) along with transductive graph-based la-
   In addition to prior knowledge about inter-               bel inference to learn a new distance metric from
instance distances, sometimes prior information              labeled as well as unlabeled data combined. In
about the matrix A, denoted by A0 , itself may               self-training styled iterations, IDML alternates be-
also be available. For example, Euclidean dis-               tween metric learning and label inference; with
tance (i.e., A0 = I) may work well in some do-               output of label inference used during next round
mains. In such cases, we would like the learned              of metric learning, and so on.
matrix A to be as close as possible to the prior ma-             IDML starts out with the assumption that ex-
trix A0 . ITML combines these two types of prior             isting supervised metric learning algorithms, such
information, i.e., knowledge about inter-instance            as ITML, can learn a better metric if the number
distances, and prior matrix A0 , in order to learn           of available labeled instances is increased. Since
the matrix A by solving the optimization problem             we are focusing on the semi-supervised learning
shown in (2).                                                (SSL) setting with nl labeled and nu unlabeled
                                                             instances, the idea is to automatically label the
min      Dld (A, A0 )                            (2)         unlabeled instances using a graph based SSL al-
A0
                                                             gorithm, and then include instances with low as-
s.t.     tr{A(xi âˆ’ xj )(xi âˆ’ xj )> } â‰¤ u,                    signed label entropy (i.e., high confidence label
                                        âˆ€(i, j) âˆˆ S          assignments) in the next round of metric learning.
                                  >                          The number of instances added in each iteration
         tr{A(xi âˆ’ xj )(xi âˆ’ xj ) } â‰¥ l,
                                        âˆ€(i, j) âˆˆ D          depends on the threshold Î² 1 . This process is con-
                                                             tinued until no new instances can be added to the
where Dld (A, A0 ) = tr(AAâˆ’1 0 ) âˆ’ log det(AA0 )
                                                 âˆ’1
                                                             set of labeled instances, which can happen when
âˆ’n, is the LogDet divergence.                                either all the instances are already exhausted, or
   To handle situations where exactly solving the            when none of the remaining unlabeled instances
problem in (2) is not possible, slack variables may          can be assigned labels with high confidence.
be introduced to the ITML objective. To solve this               The IDML framework is presented in Algo-
optimization problem, an algorithm involving re-             rithm 1. In Line 3, any supervised metric
peated Bregman projections is presented in (Davis            learner, such as ITML, may be used as the
et al., 2007), which we use for the experiments re-          M ETRIC L EARNER. Using the distance metric
ported in this paper.                                        learned in Line 3, a new k-NN graph is constructed
                                                             in Line 4 , whose edge weight matrix is stored in
2.3    Inference-Driven Metric Learning
                                                             W . In Line 5 , G RAPH L ABEL I NF optimizes over
       (IDML): Semi-Supervised
                                                             the newly constructed graph, the GRF objective
Notations: We first define the necessary notations.          (Zhu et al., 2003) shown in (3).
Let X be the d Ã— n matrix of n instances in a                                    0>       0                       0
d-dimensional space. Out of the n instances, nl                     min
                                                                      0
                                                                        tr{YÌ‚         LYÌ‚ }, s.t. SÌ‚ YÌ‚ = SÌ‚ YÌ‚       (3)
                                                                     YÌ‚
instances are labeled, while the remaining nu in-
stances are unlabeled, with n = nl + nu . Let S be           where L = D âˆ’ W is the (unnormalized) Lapla-
a n Ã— n diagonal matrix with Sii = 1 iff instance               1
                                                                    During the experiments in Section 3, we set Î² = 0.05


                                                       378


 Algorithm 1: Inference Driven Metric Learn-           first four datasets â€“ Electronics, Books, Kitchen,
 ing (IDML)                                            and DVDs â€“ are from the sentiment domain and
 Input: instances X, training labels Y , training      previously used in (Blitzer et al., 2007). WebKB
 instance indicator S, label entropy threshold Î²,      is a text classification dataset derived from (Sub-
 neighborhood size k                                   ramanya and Bilmes, 2008). For details regard-
 Output: Mahalanobis distance parameter A              ing features and data pre-processing, we refer the
                                                       reader to the origin of these datasets cited above.
     1: YÌ‚ â† Y , SÌ‚ â† S
                                                       One extra preprocessing that we did was that we
     2: repeat
                                                       only considered features which occurred more 20
     3:    A â† M ETRIC L EARNER(X, SÌ‚, YÌ‚ )
                                                       times in the entire dataset to make the problem
     4:    W â† C ONSTRUCT K NN G RAPH(X, A, k)
              0                                        more computationally tractable and also since the
     5:    YÌ‚ â† G RAPH L ABEL I NF(W, SÌ‚, YÌ‚ )
                                             0         infrequently occurring features usually contribute
     6:    U â† S ELECT L OW E NT I NST(YÌ‚ , SÌ‚, Î²)
                          0                            noise. We use classification error (lower is better)
     7:    YÌ‚ â† YÌ‚ + U YÌ‚
                                                       as the evaluation metric. We experiment with the
     8:    SÌ‚ â† SÌ‚ + U
                                                       following ways of estimating transformation ma-
     9: until convergence (i.e., Uii = 0, âˆ€i)
                                                       trix P :
    10: return A
                                                             Original2 : We set P = I, where I is the
                                                             d Ã— d identity matrix. Hence, the data is not
cian, and D is a diagonal matrix with Dii =                  transformed in this case.
P                                            0
   j Wij . The constraint, SÌ‚ YÌ‚ = SÌ‚ YÌ‚ , in (3)
makes sure that labels on training instances are not         RP: The data is first projected into a lower
changed during inference. In Line 6, a currently             dimensional space using the Random Pro-
unlabeled instance xi (i.e., SÌ‚ii = 0) is consid-            jection (RP) method (Bingham and Mannila,
ered a new labeled training instance, i.e., Uii = 1,         2001). Dimensionality of the target space
                                                                           0
for next round of metric learning if the instance            was set at d = 2log      n
                                                                                    log 1
                                                                                           , as prescribed in
has been assigned labels with high confidence in             (Bingham and Mannila, 2001). We use the
the current iteration, i.e., if its label distribution       projection matrix constructed by RP as P . 
                                       0
has low entropy (i.e., E NTROPY(YÌ‚i: ) â‰¤ Î²). Fi-             was set to 0.25 for the experiments in Sec-
nally in Line 7, training instance label information         tion 3, which has the effect of projecting the
is updated. This iterative process is continued till         data into a much lower dimensional space
no new labeled instance can be added, i.e., when             (84 for the experiments in this section). This
Uii = 0 âˆ€i. IDML returns the learned matrix A                presents an interesting evaluation setting as
which can be used to compute Mahalanobis dis-                we already run evaluations in much higher di-
tance using Equation 1.                                      mensional space (e.g., Original).

3     Experiments                                                  PCA: Data instances are first projected into
                                                                   a lower dimensional space using Principal
3.1    Setup                                                       Components Analysis (PCA) (Jolliffe, 2002)
                                                                   . Following (Weinberger and Saul, 2009), di-
         Dataset     Dimension     Balanced                        mensionality of the projected space was set
       Electronics     84816         Yes                           at 250 for all experiments. In this case, we
         Books        139535         Yes                           used the projection matrix generated by PCA
        Kitchen        73539         Yes                           as P .
         DVDs         155465         Yes
        WebKB          44261         Yes                           ITML: A is learned by applying ITML (see
                                                                   Section 2.2) on the Original space (above),
Table 1: Description of the datasets used in Sec-                  and then we decompose A as A = P > P to
tion 3. All datasets are binary with 1500 total in-                obtain P .
stances in each.                                               2
                                                                 Note that â€œOriginalâ€ in the results tables refers to orig-
                                                            inal space with features occurring more than 20 times. We
                                                            also ran experiments with original set of features (without
 Description of the datasets used during experi-            any thresholding) and the results were worse or comparable
ments in Section 3 are presented in Table 1. The            to the ones reported in the tables.


                                                      379


               Datasets      Original       RP                 PCA         ITML        IDML-IT
                              ÂµÂ±Ïƒ          ÂµÂ±Ïƒ                ÂµÂ±Ïƒ          ÂµÂ±Ïƒ           ÂµÂ±Ïƒ
              Electronics   31.3 Â± 0.9   42.5 Â± 1.0         46.4 Â± 2.0   33.0 Â± 1.0    30.7Â±0.7
                Books       37.5 Â± 1.1   45.0 Â± 1.1         34.8 Â± 1.4   35.0 Â± 1.1    32.0Â±0.9
               Kitchen      33.7 Â± 1.0   43.0 Â± 1.1         34.0 Â± 1.6   30.9 Â± 0.7    29.0Â±1.0
                DVDs        39.0 Â± 1.2   47.7 Â± 1.2         36.2 Â± 1.6   37.0 Â± 0.8    33.9Â±1.0
               WebKB        31.4 Â± 0.9   33.0 Â± 1.0         27.9 Â± 1.3   28.9 Â± 1.0    25.5Â±1.0

Table 2: Comparison of SVM % classification errors (lower is better), with 50 labeled instances (Sec.
3.2). nl =50. and nu = 1450. All results are averaged over ten trials. All hyperparameters are tuned on a
separate random split.

               Datasets      Original       RP                 PCA         ITML        IDML-IT
                              ÂµÂ±Ïƒ          ÂµÂ±Ïƒ                ÂµÂ±Ïƒ          ÂµÂ±Ïƒ           ÂµÂ±Ïƒ
              Electronics   27.0 Â± 0.9   40.0 Â± 1.0         41.2 Â± 1.0   27.5 Â± 0.8    25.3Â±0.8
                Books       31.0 Â± 0.7   42.9 Â± 0.6         31.3 Â± 0.7   29.9 Â± 0.5    27.7Â±0.7
               Kitchen      26.3 Â± 0.5   41.9 Â± 0.7         27.0 Â± 0.9   26.1 Â± 0.8    24.8Â±0.9
                DVDs        34.7 Â± 0.4   46.8 Â± 0.6         32.9 Â± 0.8   34.0 Â± 0.8    31.8Â±0.9
               WebKB        25.7 Â± 0.5   31.1 Â± 0.5         24.9 Â± 0.6   25.6 Â± 0.4    23.9Â±0.4

Table 3: Comparison of SVM % classification errors (lower is better), with 100 labeled instances (Sec.
3.2). nl =100. and nu = 1400. All results are averaged over ten trials. All hyperparameters are tuned on
a separate random split.


      IDML-IT: A is learned by applying IDML                labeled instances are shown in Table 2, and Ta-
      (Algorithm 1) (see Section 2.3) on the Orig-          ble 3, respectively. From these results, we observe
      inal space (above); with ITML used as                 that IDML-IT consistently achieves the best per-
      M ETRIC L EARNER in IDML (Line 3 in Al-               formance across all experimental settings. We also
      gorithm 1). In this case, we treat the set of         note that in Table 3, performance difference be-
      test instances (without their gold labels) as         tween ITML and IDML-IT in the Electronics and
      the unlabeled data. In other words, we essen-         Kitchen domains are statistically significant.
      tially work in the transductive setting (Vap-
      nik, 2000). Once again, we decompose A as
                                                            3.3   Semi-Supervised Classification
      A = P > P to obtain P .
   We also experimented with the supervised                 In this section, we trained the GRF classifier (see
large-margin metric learning algorithm (LMNN)               Equation 3), a graph-based semi-supervised learn-
presented in (Weinberger and Saul, 2009). We                ing (SSL) algorithm (Zhu et al., 2003), using
found ITML to be more effective in practice than            Gaussian kernel parameterized by A = P > P to
LMNN, and hence we report results based on                  set edge weights. During graph construction, each
ITML only. Each input instance, x, is now pro-              node was connected to its k nearest neighbors,
jected into the transformed space as P x. We                with k treated as a hyperparameter and tuned on
now train different classifiers on this transformed         a separate development set. Experimental results
space. All results are averaged over ten random             with 50 and 100 labeled instances are shown in
trials.                                                     Table 4, and Table 5, respectively. As before, we
                                                            experimented with nl = 50 and nl = 100. Once
3.2   Supervised Classification                             again, we observe that IDML-IT is the most effec-
We train a SVM classifier, with an RBF kernel, on           tive method, with the GRF classifier trained on the
the transformed space generated by the projection           data representation learned by IDML-IT achieving
matrix P . SVM hyperparameter, C and RBF ker-               best performance in all settings. Here also, we ob-
nel bandwidth, were tuned on a separate develop-            serve that IDML-IT achieves the best performance
ment split. Experimental results with 50 and 100            across all experimental settings.


                                                      380


              Datasets      Original        RP                 PCA           ITML         IDML-IT
                             ÂµÂ±Ïƒ           ÂµÂ±Ïƒ                ÂµÂ±Ïƒ            ÂµÂ±Ïƒ            ÂµÂ±Ïƒ
             Electronics   47.9 Â± 1.1    49.0 Â± 1.2         43.2 Â± 0.9     34.9 Â± 0.5     34.0Â±0.5
               Books       50.0 Â± 1.0    49.4 Â± 1.0         47.9 Â± 0.7     42.1 Â± 0.7     40.6Â±0.7
              Kitchen      49.8 Â± 1.1    49.6 Â± 0.9         48.6 Â± 0.8     31.1 Â± 0.5     30.0Â±0.5
               DVDs        50.1 Â± 0.5    49.9 Â± 0.7         49.4 Â± 0.6     42.1 Â± 0.4     41.2Â±0.5
              WebKB        33.1 Â± 0.4    33.1 Â± 0.3         33.1 Â± 0.3     30.0 Â± 0.4     28.7Â±0.5

Table 4: Comparison of transductive % classification errors (lower is better) over graphs constructed
using different methods (see Section 3.3), with nl = 50 and nu = 1450. All results are averaged over
ten trials. All hyperparameters are tuned on a separate random split.

              Datasets      Original        RP                 PCA           ITML         IDML-IT
                             ÂµÂ±Ïƒ           ÂµÂ±Ïƒ                ÂµÂ±Ïƒ            ÂµÂ±Ïƒ            ÂµÂ±Ïƒ
             Electronics   43.5 Â± 0.7    47.2 Â± 0.8         39.1 Â± 0.7     31.3 Â± 0.2     30.8Â±0.3
               Books       48.3 Â± 0.5    48.9 Â± 0.3         43.3 Â± 0.4     35.2 Â± 0.5     33.3Â±0.6
              Kitchen      45.3 Â± 0.6    48.2 Â± 0.5         41.0 Â± 0.7     30.7 Â± 0.6     29.9Â±0.3
               DVDs        48.6 Â± 0.3    49.3 Â± 0.5         45.9 Â± 0.5     42.6 Â± 0.4     41.7Â±0.3
              WebKB        33.4 Â± 0.4    33.4 Â± 0.4         33.4 Â± 0.3     30.4 Â± 0.5     28.6Â±0.7

Table 5: Comparison of transductive % classification errors (lower is better) over graphs constructed
using different methods (see Section 3.3), with nl = 100 and nu = 1400. All results are averaged over
ten trials. All hyperparameters are tuned on a separate random split.


4   Conclusion                                              References
                                                            E. Bingham and H. Mannila. 2001. Random projec-
In this paper, we compared the effectiveness                   tion in dimensionality reduction: applications to im-
of the transformed spaces learned by recently                  age and text data. In ACM SIGKDD.
proposed supervised, and semi-supervised metric
                                                            J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra-
learning algorithms to those generated by previ-               phies, bollywood, boom-boxes and blenders: Do-
ously proposed unsupervised dimensionality re-                 main adaptation for sentiment classification. In
duction methods (e.g., PCA). To the best of our                ACL.
knowledge, this is the first study of its kind in-          J.V. Davis, B. Kulis, P. Jain, S. Sra, and I.S. Dhillon.
volving NLP datasets. Through a variety of ex-                 2007. Information-theoretic metric learning. In
periments on different real-world NLP datasets,                ICML.
we demonstrated that supervised as well as semi-            P. S. Dhillon, P. P. Talukdar, and K. Crammer. 2010.
supervised classifiers trained on the space learned            Inference-driven metric learning for graph construc-
by IDML-IT consistently result in the lowest clas-             tion. Technical Report MS-CIS-10-18, CIS Depart-
sification errors. Encouraged by these early re-               ment, University of Pennsylvania, May.
sults, we plan to explore further the applicability         IT Jolliffe. 2002.     Principal component analysis.
of IDML-IT in other NLP tasks (e.g., entity classi-           Springer verlag.
fication, word sense disambiguation, polarity lexi-         A. Subramanya and J. Bilmes. 2008. Soft-Supervised
con induction, etc.) where better representation of           Learning for Text Classification. In EMNLP.
the data is a pre-requisite for effective learning.
                                                            V.N. Vapnik. 2000. The nature of statistical learning
                                                              theory. Springer Verlag.
Acknowledgments
                                                            K.Q. Weinberger and L.K. Saul. 2009. Distance metric
Thanks to Kuzman Ganchev for providing detailed               learning for large margin nearest neighbor classifica-
                                                              tion. The Journal of Machine Learning Research.
feedback on a draft of this paper. This work
was supported in part by NSF IIS-0447972 and                X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
DARPA HRO1107-1-0029.                                         supervised learning using Gaussian fields and har-
                                                              monic functions. In ICML.



                                                      381
